{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/trainer/blob/main/crawler.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "def fetch_image_urls_from_subreddit(subreddit, num_links, after):\n",
        "    url = f\"https://www.reddit.com/r/{subreddit}/new/.json?limit={num_links}&after={after}\"\n",
        "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        return [post['data']['url'] for post in data['data']['children']], data['data']['after']\n",
        "    else:\n",
        "        print(Fore.RED + f\"Error fetching image URLs: {response.status_code} - {response.text}\")\n",
        "        return []\n",
        "\n",
        "items = []\n",
        "after = \"\"\n",
        "sub_reddit = \"trueratecelebrities\"\n",
        "\n",
        "for i in range(10):\n",
        "  new_images, after = fetch_image_urls_from_subreddit(sub_reddit, 100, after)\n",
        "  items.extend(new_images)\n",
        "  print(after)\n",
        "\n",
        "!mkdir /content/images\n",
        "images = []\n",
        "\n",
        "for item in items:\n",
        "    headers={'user-agent': 'Mozilla/5.0'}\n",
        "    try:\n",
        "      r=requests.get(item, headers=headers)\n",
        "      name = item.split('/')[-1]\n",
        "      with open(f\"/content/images/{name}\", 'wb') as f:\n",
        "        f.write(r.content)\n",
        "        images.append(f\"/content/images/{name}\")\n",
        "    except Exception as e:\n",
        "      pass\n",
        "\n",
        "print(len(images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!pip install https://github.com/camenduru/insightface-colab/releases/download/v0.1/mmcv_full-1.4.0-cp310-cp310-linux_x86_64.whl\n",
        "!pip install gradio insightface==0.6.2 terminaltables==3.1.0 mmpycocotools==12.0.3\n",
        "!git clone -b dev --recurse-submodules https://github.com/camenduru/insightface-SCRFD-hf\n",
        "%cd /content/insightface-SCRFD-hf\n",
        "!sed -i '23,26d' /content/insightface-SCRFD-hf/insightface/detection/scrfd/mmdet/__init__.py\n",
        "!wget https://huggingface.co/public-data/insightface/resolve/main/models/scrfd_34g/model.pth -O /content/model.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content/insightface-SCRFD-hf/insightface/detection/scrfd\n",
        "\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from mmdet.apis import inference_detector, init_detector, show_result_pyplot\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = init_detector(\"/content/insightface-SCRFD-hf/insightface/detection/scrfd/configs/scrfd/scrfd_34g.py\", \"/content/model.pth\", 'cpu')\n",
        "\n",
        "def detect(image: np.ndarray) -> np.ndarray:\n",
        "    face_score_threshold = 0.5\n",
        "    \n",
        "    image = image[:, :, ::-1]\n",
        "    preds = inference_detector(model, image)\n",
        "    boxes = preds[0]\n",
        "\n",
        "    res = image.copy()\n",
        "    for box in boxes:\n",
        "        box, score = box[:4], box[4]\n",
        "        if score < face_score_threshold:\n",
        "            continue\n",
        "        box = np.round(box).astype(int)\n",
        "        line_width = max(2, int(3 * (box[2:] - box[:2]).max() / 256))\n",
        "        # cv2.rectangle(res, tuple(box[:2]), tuple(box[2:]), (0, 255, 0), line_width)\n",
        "\n",
        "        center_x = (box[0] + box[2]) // 2\n",
        "        center_y = (box[1] + box[3]) // 2\n",
        "        center = (center_x, center_y)\n",
        "        # cv2.circle(res, center, 5, (0, 0, 255), -1)\n",
        "\n",
        "        length = math.sqrt(box[2]**2 + box[3]**2)\n",
        "        x = int(center_x - int(length//2))\n",
        "        y = int(center_y - int(length//2))\n",
        "\n",
        "        if x < 0:\n",
        "            x = 0\n",
        "        if y < 0:\n",
        "            y = 0\n",
        "        if x + int(length) > res.shape[1]:\n",
        "            x = res.shape[1] - int(length)\n",
        "        if y + int(length) > res.shape[0]:\n",
        "            y = res.shape[0] - int(length)\n",
        "\n",
        "        width, height = int(length), int(length)\n",
        "        x = center_x - (width // 2)\n",
        "        y = center_y - (height // 2)\n",
        "\n",
        "        if x < 0:\n",
        "            x = 0\n",
        "        if y < 0:\n",
        "            y = 0\n",
        "        if x + width > res.shape[1]:\n",
        "            x = res.shape[1] - width\n",
        "        if y + height > res.shape[0]:\n",
        "            y = res.shape[0] - height\n",
        "\n",
        "        res = res[y:y+height, x:x+width]\n",
        "        res = cv2.resize(res, (512, 512))\n",
        "\n",
        "    res = cv2.cvtColor(res, cv2.COLOR_BGR2RGB)\n",
        "    return res\n",
        "\n",
        "!mkdir /content/cropped\n",
        "directory = '/content/images'\n",
        "image_paths = []\n",
        "for root, _, files in os.walk(directory):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
        "            image_path = os.path.join(root, file)\n",
        "            image_paths.append(image_path)\n",
        "\n",
        "for image_path in image_paths:\n",
        "    try:\n",
        "      image_array = np.array(Image.open(image_path))\n",
        "      image = Image.fromarray(detect(image_array))\n",
        "      image_name = os.path.basename(image_path)\n",
        "      image.save(f'/content/cropped/{image_name}')\n",
        "    except Exception as e:\n",
        "      print(f\"{image_path}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaAJk33ppFw1"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "\n",
        "!apt -y update -qq\n",
        "!wget https://github.com/camenduru/gperftools/releases/download/v1.0/libtcmalloc_minimal.so.4 -O /content/libtcmalloc_minimal.so.4\n",
        "%env LD_PRELOAD=/content/libtcmalloc_minimal.so.4\n",
        "%env TF_CPP_MIN_LOG_LEVEL=1\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "!pip install -q torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 torchtext==0.15.2 torchdata==0.6.1 --extra-index-url https://download.pytorch.org/whl/cu118 -U\n",
        "!pip install -q xformers==0.0.20 triton==2.0.0 diffusers==0.19.0 datasets==2.14.0 gradio==3.38.0 wandb==0.15.7 transformers==4.26.0 accelerate==0.16.0 bitsandbytes==0.41.0 -U\n",
        "\n",
        "!git clone https://github.com/camenduru/trainer\n",
        "\n",
        "diffusers_version = \"v0.19.0\"\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://raw.githubusercontent.com/huggingface/diffusers/{diffusers_version}/scripts/convert_diffusers_to_original_stable_diffusion.py -d /content/trainer/diffusers/dreambooth -o convert_diffusers_to_original_stable_diffusion.py\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://raw.githubusercontent.com/huggingface/diffusers/{diffusers_version}/examples/dreambooth/train_dreambooth.py -d /content/trainer/diffusers/dreambooth -o train_dreambooth.py\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://raw.githubusercontent.com/huggingface/diffusers/{diffusers_version}/examples/dreambooth/train_dreambooth_lora.py -d /content/trainer/diffusers/lora -o train_dreambooth_lora.py\n",
        "\n",
        "BaseModelUrl = \"https://huggingface.co/uf/cyberrealistic_v3.2\"\n",
        "BaseModelDir = \"/content/model\"\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {BaseModelUrl}/raw/main/model_index.json -d {BaseModelDir} -o model_index.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {BaseModelUrl}/resolve/main/vae/diffusion_pytorch_model.bin -d {BaseModelDir}/vae -o diffusion_pytorch_model.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {BaseModelUrl}/raw/main/vae/config.json -d {BaseModelDir}/vae -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {BaseModelUrl}/resolve/main/unet/diffusion_pytorch_model.bin -d {BaseModelDir}/unet -o diffusion_pytorch_model.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {BaseModelUrl}/raw/main/unet/config.json -d {BaseModelDir}/unet -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {BaseModelUrl}/raw/main/tokenizer/vocab.json -d {BaseModelDir}/tokenizer -o vocab.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {BaseModelUrl}/raw/main/tokenizer/tokenizer_config.json -d {BaseModelDir}/tokenizer -o tokenizer_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {BaseModelUrl}/raw/main/tokenizer/special_tokens_map.json -d {BaseModelDir}/tokenizer -o special_tokens_map.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {BaseModelUrl}/raw/main/tokenizer/merges.txt -d {BaseModelDir}/tokenizer -o merges.txt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {BaseModelUrl}/resolve/main/text_encoder/pytorch_model.bin -d {BaseModelDir}/text_encoder -o pytorch_model.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {BaseModelUrl}/raw/main/text_encoder/config.json -d {BaseModelDir}/text_encoder -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {BaseModelUrl}/raw/main/scheduler/scheduler_config.json -d {BaseModelDir}/scheduler -o scheduler_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {BaseModelUrl}/resolve/main/safety_checker/pytorch_model.bin -d {BaseModelDir}/safety_checker -o pytorch_model.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {BaseModelUrl}/raw/main/safety_checker/config.json -d {BaseModelDir}/safety_checker -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {BaseModelUrl}/raw/main/feature_extractor/preprocessor_config.json -d {BaseModelDir}/feature_extractor -o preprocessor_config.json\n",
        "\n",
        "%cd /content/trainer\n",
        "!python realistic.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
